---
title: "Bayesian data analysis of Tug of War model"
author: "Michael Henry Tessler, Tobias Gerstenberg, Patrick Wahl"
date: "Decemer 12, 2016"
output: html_document
---

```{r}
library(knitr)
knitr::opts_chunk$set(fig.crop = F,echo=T, 
                      warning=F, cache=F, 
                      message=F, sanitize = T)

library(rwebppl)
library(dplyr)
library(tidyr)
library(coda)
library(ggplot2)
library(jsonlite)

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}
hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}
hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
```

# Bayesian Data Analysis of Tug of War model

Currently, this document has a sketch of what the data analysis will look like for the tug of war experiments.
Overall, we have a Bayesian cognitive model of the tug-of-war task (previously from Gerstenberg & Goodman (2012), cogsci proceedings) and a Bayesian data analysis model that puts uncertainty over the cognitive model parameters.
This is an inference-about-inference (or, nested Bayesian) model and could pose computational challenges when we deal with the experimental data.
We try to foresee some of the issues and spell out our tentative approach at this time.

## Tug of War data

First, we'll load the raw tug of war data, analyzed in `tug_analysis.R`.

```{r}
#source("../analysis/tug_analysis.R")
load("../../../data/exp1_data.RData")
# df.games has the detailed match info
# df.info has more high level info
```

What will the data look like in WebPPL?

```{r echo=T}
toJSON(head(df.info, 2), pretty = T)
toJSON(head(df.games, 2), pretty = T)
toJSON(head(df.long, 2), pretty = T)
```

Here, we see that the data frame will be converted into an array of objects. 

We write the tug-of-war cognitive model and the Bayesian data analysis model in WebPPL.

#### Helper functions


In the behavioral data, we have codes for different tournament configurations.
This is coming from `df.games`, which we repackage insid webppl into `tournamentInfo`


```{r tournamentInfo, echo = F}
tournamentInfo <-' 
var dfGames = towData.dfGames;
var dfLong = towData.dfLong;

var tournamentInfo = _.object(map(function(game_id){
  var games = filter(function(game){ game.id == game_id }, dfGames);
  return [game_id, games];
}, _.uniq(_.pluck(dfGames, "id"))))

// display(tournamentInfo)
// tournamentInfo
'

# rs <- webppl(tournamentInfo, data = df.games, data_var = "dfGames")
```

```{r helpers2}
helpers <- '
var levels = function(a, lvl){ return _.uniq(_.pluck(a, lvl)) }

// var outcomes = levels(towData, "outcome");

var tournaments = levels(dfGames, "id");

// bins = [0.05, 0.15,  ... , 0.95];
var bins = map(function(k){return Math.round(k*20)/20}, _.range(0.05, 0.951, 0.1));

var round = function(x){
  return bins[Math.floor(x*10)];
};

// add a tiny bit of noise, and make sure every bin has at least epsilon probability
var smoothToBins = function(dist){
  Infer({model: function(){
    var b = uniformDraw(bins);
    var scr = dist.score(b) == -Infinity ? Math.log(1/250) : dist.score(b)  // -11 = Math.log(1/50000);
    factor(scr);
    return b
  }})
}
var discretizedBeta = Infer({method: "forward", samples: 50000, model: function(){
  var x = beta(1,1);
  return round(x);
}})

// discretizedBeta
'
#table(webppl(helpers))
```

#### Tug of war Model

This model has the right structure, but some parameters may have to be changed.
In particular, the strength parameter should not be negative, and it should match the scale of the rating data.
One approach would be to take the full distribution of responses, compute the mean and stdev and use those as the mean and stdev of the strength parameter.

Also, rejection sampling (as this model is currently written) likely won't suffice for nesting within the bayesian data analysis model. Variational inference is one possible strategy. If we go with variational (and even if we don't), we may have to change the hard condition statements into `observe`. To handle this, we may imagine that a "win" is tantamout to being some number of points ahead of the opponent. 

It may look something like:

```{javascript, echo = T, eval = F}
var obsFn = function(pulling1, pulling2){
  observe(Gaussian({mu: pulling1 - pulling2, sigma: s}), k)
}
```

This is more graded version of a "win".
It says that the total strength of team1 was greater than that of team2 by `k`, assuming some noise `s`. 
`k` and `s` would be data-analytic, nuisance parameters.
This formulation might be easier for variational inference (or, MCMC / HMC).

```{r towCogModel}
towModel <- "
var tugOfWarOpts = {method: 'rejection', samples: 250};

// var tournamentInfo =  [ 
//  { id: '8', team1: ['1','2'], team2: ['3','4'], winner: '1' },
//  { id: '8', team1: ['1','3'], team2: ['2','5'], winner: '1' },
//  { id: '8', team1: ['1','4'], team2: ['3','6'], winner: '1' } 
// ];

var tugOfWarModel = function(lazyPulling, lazinessPrior, tournamentInfo){
  Infer(tugOfWarOpts, function(){

    var strength = mem(function(person){
      //return beta(5, 5)
      return sample(discretizedBeta);
    })

    var lazy = function(person){
      return flip(lazinessPrior)
    }
    var pulling = function(person) {
      return lazy(person) ?
              strength(person) * lazyPulling :
              strength(person)
    }
    var totalPulling = function(team){return sum(map(pulling, team)) }

    var winner = function(team1, team2){
      totalPulling(team1) > totalPulling(team2) ? team1 : team2
    }
    var beat = function(team1, team2){winner(team1,team2) == team1}

    map(function(matchInfo){
      var winner = matchInfo.winner;
      condition(winner == 1 ? 
                beat(matchInfo.team1, matchInfo.team2) :  
                beat(matchInfo.team2, matchInfo.team1) )
    }, tournamentInfo)

    return round(strength('1'))

  })
}

// tugOfWarModel(0.3, 0.2, tournamentInfo);
"

#qplot(webppl(paste(helpers, towModel, sep = "\n")))
```

#### Bayesian data analysis model

```{r bdaOFtowEnumerate}
bdaTow <- '
var dataAnalysisModel = function(){

   // var lazinessPrior = uniformDraw(_.range(0.1,0.51, 0.1));
   var lazinessPrior = uniformDrift({a:0, b: 0.5, width:0.1});
   // var lazyPulling = uniformDraw(_.range(0.1,1, 0.2));
   var lazyPulling = uniformDrift({a:0, b: 1, width:0.2});
   // var noise = uniformDraw(_.range(0.01,0.5, 0.2));
   display({lazyPulling, lazinessPrior});
  
  var predictions = map(function(tournament_id){
     // display(tournament_id);
      // participants ratings (towData ids offset by 1)
      var itemData = _.where(dfLong, {id: tournament_id - 1})

      // information about the winners and losers
      var matchInformation = tournamentInfo[tournament_id];

      var modelPosterior = tugOfWarModel(lazyPulling, lazinessPrior, matchInformation);
      var smoothedPredictions = smoothToBins(modelPosterior)

      var obsFn = function(d){ 
       //   display(smoothedPredictions.score(d.roundedRating) + " " + d.roundedRating);
          observe(smoothedPredictions, d.roundedRating);
      }

      mapData({data : itemData}, obsFn);

      return _.object([[tournament_id, expectation(modelPosterior)]])
  
    }, tournaments)

  return {
    parameters: {
        lazinessPrior: lazinessPrior, 
        lazyPulling: lazyPulling
    },
    predictives: _.object(_.flatten(map(function(i){ _.pairs(i) }, _.flatten(predictions)), true))
  }
}
'
```

Round data to bins

```{r}
bins <- seq(0.05, 0.95, 0.1);
df.long <- df.long %>%
  rowwise() %>%
  mutate(roundedSlot = ceiling(rating/10)) %>%
  mutate(roundedSlot = ifelse(roundedSlot == 0, 1, roundedSlot)) %>%
  mutate(roundedRating = bins[roundedSlot])
```

Run the model


```{r, eval = F}
df.tow <- list(dfGames = df.games, dfLong = df.long)

fullModel <- paste(tournamentInfo, 
                   helpers, 
                   towModel, 
                   bdaTow, sep = '\n')

posterior <- webppl(fullModel,
       data = df.tow,
       data_var = "towData",
       #inference_opts = list(method = "enumerate"),
       inference_opts = list(method = "MCMC", samples = 10, burn = 0, verbose = T),
       chains = 1,
       cores = 1,
       model_var = "dataAnalysisModel",
       output_format = "webppl")

# save(posterior, 
#      file = "~/Documents/learning/probmods2/assets/data/enumerateToW1.RData")
```

Examine parameters.

```{r}
params.tidy <- posterior %>%
  select(starts_with("parameters"), prob) %>%
  gather(key, val, -prob) %>%
  mutate(key = gsub("parameters.", "", key)) %>%
  spread(key, val)

ggplot(params.tidy, aes(x = lazinessPrior, y = lazyPulling, fill = prob))+
  geom_tile()+
  ggtitle("Posterior on parameters")
```

### Examine posterior predictive

```{r}
predictive.tidy <- posterior %>%
  select(starts_with("predictives"), prob) %>%
  gather(key, val, -prob) %>%
  mutate(key = gsub("predictives.", "", key))

predictive.summary <- predictive.tidy %>%
  group_by(key) %>%
  summarize(expval = sum(prob*val))
```

Summarize TOW Data

```{r}
library(langcog) # github.com/langcog/langcog
df.summary <- df.long %>%
  group_by(id) %>%
  multi_boot_standard(column = "rating")
```

Model-data comparison

```{r}
md.summary <- left_join(predictive.summary %>% mutate(id = as.numeric(key) - 1), 
                        df.summary)

ggplot(md.summary, aes(x = expval,
                       y = mean, ymin = ci_lower, ymax = ci_upper))+
  geom_point()+
  geom_abline(intercept = 0, slope = 1, linetype = 3)+
  xlim(0, 1)+
  ylim(0, 100)+
#  coord_fixed()+
  ylab("Human data (means)")+
  xlab("Model posterior predictive (expectation)")

with(md.summary, cor(expval, mean))^2
```

Model explains `r round(100*with(md.summary, cor(expval, mean))^2)`% of the variance
